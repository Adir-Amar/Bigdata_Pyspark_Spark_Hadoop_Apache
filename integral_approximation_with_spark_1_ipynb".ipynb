{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzuIonJUTSnpt1VDMP6DYk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShovalBenjer/Bigdata_Pyspark_Spark_Hadoop_Apache/blob/main/integral_approximation_with_spark_1_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Notebook explores distributed numerical integration using Apache Spark integrated into google colabs jupyter notebooks, focusing on approximating the integral of a specific f(x) over a specified range. By varying the number of intervals n and Spark workers, the project evaluates the trade-offs between computation accuracy, execution time, and scalability in a distributed environment. The results demonstrate how increasing n improves precision while parallelism enhances performance, highlighting the efficiency and limitations of distributed systems for computational tasks."
      ],
      "metadata": {
        "id": "HGBKg28dhhR5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zeY0wLmpahWe"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "import time\n",
        "import pandas as pd\n",
        "from sympy import symbols, integrate\n",
        "\n",
        "sc = SparkContext(\"local\", \"Integral Approximation\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    return 10 * x**2 - 2\n",
        "    \"\"\"Function to calculate f(x) = 10x^2 - 2.\"\"\"\n",
        "\n",
        "def run_experiments():\n",
        "    \"\"\"\n",
        "    Run the integral approximation for various configurations of workers and intervals.\n",
        "    Returns:\n",
        "        pd.DataFrame: Results as a DataFrame.\n",
        "    \"\"\"\n",
        "    a, b = 1, 20  # bounds of the integral\n",
        "    n_values = [100, 1000, 10000]  # number of intervals\n",
        "    worker_counts = [2, 4]  # number of workers\n",
        "    results = []"
      ],
      "metadata": {
        "id": "FlH39-_1bl9J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    \"\"\"Function to calculate f(x) = 10x^2 - 2.\"\"\"\n",
        "    return 10 * x**2 - 2\n",
        "\n",
        "def exact_integral_sympy(a, b):\n",
        "    \"\"\"\n",
        "    Run the integral approximation for various configurations of workers and intervals.\n",
        "    The function performs the following steps:\n",
        "    1. Sets the bounds of the integral (a = 1, b = 20).\n",
        "    2. Defines the number of intervals (n_values) for approximation: [100, 1000, 10000].\n",
        "    3. Defines the number of workers (worker_counts) for parallelism: [2, 4].\n",
        "    4. Iterates through combinations of intervals and worker counts to:\n",
        "        - Approximate the integral using Spark RDDs.\n",
        "        - Measure the execution time for the computation.\n",
        "        - Compare the approximate integral with the exact integral calculated using SymPy.\n",
        "        - Compute the error between the approximate and exact integral values.\n",
        "    5. Stores the results (n, number of workers, error, execution time) in a list.\n",
        "    6. Converts the results into a Pandas DataFrame for analysis.\n",
        "    Returns:\n",
        "        pd.DataFrame: Results as a DataFrame with columns:\n",
        "            - \"n\": Number of intervals used in the approximation.\n",
        "            - \"Number of Workers\": Number of workers (parallelism) used in Spark.\n",
        "            - \"Error\": Absolute error between the approximate and exact integral.\n",
        "            - \"Execution Time (s)\": Time taken to compute the integral.\n",
        "    \"\"\"\n",
        "    x = symbols('x')\n",
        "    f_sympy = 10 * x**2 - 2\n",
        "    exact_integral = integrate(f_sympy, (x, a, b))\n",
        "    return float(exact_integral)\n",
        "\n",
        "# Define the integral approximation function\n",
        "def calculate_integral(a, b, n, num_workers):\n",
        "    \"\"\"\n",
        "    Calculate the integral approximation using Spark RDDs.\n",
        "    Args:\n",
        "        a (float): Lower bound of the integral.\n",
        "        b (float): Upper bound of the integral.\n",
        "        n (int): Number of intervals.\n",
        "        num_workers (int): Number of Spark partitions (workers).\n",
        "    Returns:\n",
        "        float: Approximated integral value.\n",
        "    \"\"\"\n",
        "    h = (b - a) / n  # Step size\n",
        "    x_values = [a + k * h for k in range(n + 1)]  # Divide the interval into n steps\n",
        "    rdd = sc.parallelize(x_values, num_workers)  # Distribute data among workers\n",
        "\n",
        "    # Calculate the sum using the Spark RDD map and reduce\n",
        "    integral_sum = rdd.map(f).reduce(lambda x, y: x + y)\n",
        "\n",
        "    # Apply the formula for the integral approximation\n",
        "    result = h * ((f(a) + f(b)) / 2 + integral_sum - f(a) - f(b))\n",
        "    return result\n",
        "\n",
        "# Run the integral calculation for different configurations\n",
        "def run_experiments():\n",
        "    \"\"\"\n",
        "    Run the integral approximation for various configurations of workers and intervals.\n",
        "    The function performs the following steps:\n",
        "    1. Sets the bounds of the integral (a = 1, b = 20).\n",
        "    2. Defines the number of intervals (n_values) for approximation: [100, 1000, 10000].\n",
        "    3. Defines the number of workers (worker_counts) for parallelism: [2, 4].\n",
        "    4. Iterates through combinations of intervals and worker counts to:\n",
        "        - Approximate the integral using Spark RDDs.\n",
        "        - Measure the execution time for the computation.\n",
        "        - Compare the approximate integral with the exact integral calculated using SymPy.\n",
        "        - Compute the error between the approximate and exact integral values.\n",
        "    5. Stores the results (n, number of workers, error, execution time) in a list.\n",
        "    6. Converts the results into a Pandas DataFrame for analysis.\n",
        "    Returns:\n",
        "        pd.DataFrame: Results as a DataFrame with columns:\n",
        "            - \"n\": Number of intervals used in the approximation.\n",
        "            - \"Number of Workers\": Number of workers (parallelism) used in Spark.\n",
        "            - \"Error\": Absolute error between the approximate and exact integral.\n",
        "            - \"Execution Time (s)\": Time taken to compute the integral.\n",
        "    \"\"\"\n",
        "    a, b = 1, 20\n",
        "    n_values = [100, 1000, 10000]\n",
        "    worker_counts = [2, 4]\n",
        "    results = []\n",
        "\n",
        "    for num_workers in worker_counts:\n",
        "        for n in n_values:\n",
        "            start_time = time.time()\n",
        "            integral_value = calculate_integral(a, b, n, num_workers)\n",
        "            execution_time = time.time() - start_time\n",
        "            expected_value = exact_integral_sympy(a, b)\n",
        "            error = abs(integral_value - expected_value)\n",
        "            results.append((n, num_workers, error, execution_time))\n",
        "    df = pd.DataFrame(results, columns=[\"n\", \"Number of Workers\", \"Error\", \"Execution Time (s)\"])\n",
        "    return df"
      ],
      "metadata": {
        "id": "WZKiOU1tfGkA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = run_experiments()\n",
        "results_sorted = results_df.sort_values(by=[\"Number of Workers\", \"n\"])\n",
        "print(results_sorted)\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrretjOffLSV",
        "outputId": "668e6c96-232d-4b5f-944b-8b975d556c84"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       n  Number of Workers     Error  Execution Time (s)\n",
            "0    100                  2  1.143167            2.545882\n",
            "1   1000                  2  0.011432            0.575953\n",
            "2  10000                  2  0.000114            0.424423\n",
            "3    100                  4  1.143167            0.970460\n",
            "4   1000                  4  0.011432            0.823609\n",
            "5  10000                  4  0.000114            0.933813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| \\( n \\)   | Number of Workers | Error       | Execution Time (s) | Explanation                                                                 |\n",
        "|-----------|-------------------|-------------|---------------------|-----------------------------------------------------------------------------|\n",
        "| 100       | 2                 | 1.143167    | 2.545882            | High error due to fewer intervals (\\( n \\)) leading to less precise results.|\n",
        "| 1000      | 2                 | 0.011432    | 0.575953            | Error significantly reduced with more intervals, but execution time drops. |\n",
        "| 10000     | 2                 | 0.000114    | 0.424423            | Very low error with higher intervals, faster execution due to distributed load.|\n",
        "| 100       | 4                 | 1.143167    | 0.970460            | High error persists, but adding workers reduces execution time significantly.|\n",
        "| 1000      | 4                 | 0.011432    | 0.823609            | Lower error, but slight increase in execution time due to distributed overhead.|\n",
        "| 10000     | 4                 | 0.000114    | 0.933813            | Minimal error, execution time slightly increases due to coordination overhead.|\n"
      ],
      "metadata": {
        "id": "UBw6zfBEgwQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing n (number of intervals) reduces error significantly, with  n=10000 achieving near-perfect accuracy. Adding more workers improves execution time for smaller n, but coordination overhead slightly impacts performance for larger\n",
        "n.\n",
        "**Optimal results are achieved with n=10000 and 2 workers for a balance of accuracy and efficiency.**"
      ],
      "metadata": {
        "id": "Wqe3IlvMhI_X"
      }
    }
  ]
}